#!/usr/bin/env python3
"""TLG Text Extraction Script.

Extracts Greek texts from all discovered TLG works using the catalogue
generated by discover_tlg.py.

Adapted from proven Daphnet extraction patterns with TLG-specific optimizations.
"""

from __future__ import annotations

import asyncio
import json
import logging
import re
import unicodedata
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Set

import aiohttp
import yaml
from bs4 import BeautifulSoup, NavigableString

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Paths
CONFIG_FILE = Path(__file__).parent / "config.yaml"
OUTPUT_DIR = Path(__file__).parent / "data" / "tlg"
CATALOGUE_FILE = OUTPUT_DIR / "tlg_catalogue.json"
CHECKPOINT_DIR = OUTPUT_DIR / "checkpoints"

# Load config
with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
    CONFIG = yaml.safe_load(f)

@dataclass
class ExtractedText:
    """Extracted TLG text with metadata."""
    tlg_id: str
    work_id: str
    author_name: str
    work_title: str
    greek_text: str
    paragraphs: List[str]
    url: str
    extraction_date: str
    greek_ratio: float
    word_count: int
    extraction_method: str

class TLGExtractor:
    """Extract Greek texts from TLG corpus using discovered catalogue."""
    
    def __init__(self):
        self.session: Optional[aiohttp.ClientSession] = None
        self.results: List[ExtractedText] = []
        self.processed_works: Set[str] = set()
        self.catalogue = self.load_catalogue()
        
    def load_catalogue(self) -> Dict:
        """Load the TLG catalogue generated by discovery."""
        if not CATALOGUE_FILE.exists():
            raise FileNotFoundError(f"Catalogue not found: {CATALOGUE_FILE}")
            
        with open(CATALOGUE_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    async def setup(self):
        """Initialize HTTP session."""
        headers = {
            'User-Agent': CONFIG["source"]["user_agent"]
        }
        timeout = aiohttp.ClientTimeout(total=CONFIG["extraction"]["timeout"])
        self.session = aiohttp.ClientSession(headers=headers, timeout=timeout)
    
    async def cleanup(self):
        """Clean up resources."""
        if self.session:
            await self.session.close()
    
    def is_greek_text(self, text: str) -> bool:
        """Validate that text contains substantial Greek content."""
        if not text or len(text) < CONFIG["extraction"]["content_validation"]["min_text_length"]:
            return False
            
        greek_chars = 0
        total_chars = 0
        
        for char in text:
            if char.isalpha():
                total_chars += 1
                # Greek Unicode ranges
                if ('\u0370' <= char <= '\u03FF' or  # Greek and Coptic
                    '\u1F00' <= char <= '\u1FFF'):   # Greek Extended
                    greek_chars += 1
        
        if total_chars == 0:
            return False
            
        greek_ratio = greek_chars / total_chars
        return greek_ratio >= CONFIG["extraction"]["content_validation"]["greek_threshold"]
    
    def normalize_greek(self, text: str) -> str:
        """Normalize Greek text to NFC form."""
        return unicodedata.normalize('NFC', text)
    
    def extract_greek_from_html(self, html_content: str) -> Optional[Dict[str, str]]:
        """Extract Greek text from TLG HTML content."""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Remove navigation and metadata elements
        for element in soup.find_all(['script', 'style', 'nav', 'header', 'footer']):
            element.decompose()
        
        greek_text = ""
        paragraphs = []
        method = "unknown"
        
        # Try primary selectors first
        for selector in CONFIG["extraction"]["selectors"]["primary"]:
            elements = soup.select(selector)
            if elements:
                for elem in elements:
                    text = elem.get_text(separator=' ', strip=True)
                    if self.is_greek_text(text):
                        greek_text += text + "\\n\\n"
                        paragraphs.extend([p.strip() for p in text.split('\\n') if p.strip()])
                        method = f"primary_selector:{selector}"
                        break
                if greek_text:
                    break
        
        # Try fallback selectors if primary failed
        if not greek_text:
            for selector in CONFIG["extraction"]["selectors"]["fallback"]:
                elements = soup.select(selector)
                if elements:
                    for elem in elements:
                        text = elem.get_text(separator=' ', strip=True)
                        if self.is_greek_text(text):
                            greek_text += text + "\\n\\n"
                            paragraphs.extend([p.strip() for p in text.split('\\n') if p.strip()])
                            method = f"fallback_selector:{selector}"
                            break
                    if greek_text:
                        break
        
        # Final attempt: extract all text and validate
        if not greek_text:
            all_text = soup.get_text(separator=' ', strip=True)
            if self.is_greek_text(all_text):
                greek_text = all_text
                paragraphs = [p.strip() for p in all_text.split('\\n') if p.strip()]
                method = "full_text_extraction"
        
        if not greek_text:
            return None
            
        # Normalize and clean
        greek_text = self.normalize_greek(greek_text.strip())
        paragraphs = [self.normalize_greek(p) for p in paragraphs if p]
        
        return {
            'greek_text': greek_text,
            'paragraphs': paragraphs,
            'method': method
        }
    
    async def extract_work(self, author: Dict, work: Dict) -> Optional[ExtractedText]:
        """Extract text from a single TLG work."""
        work_key = f"{author['tlg_id']}_{work['work_id']}"
        
        if work_key in self.processed_works:
            logger.debug(f"Work {work_key} already processed")
            return None
        
        url = work['url']
        
        for attempt in range(CONFIG["extraction"]["max_retries"]):
            try:
                async with self.session.get(url) as response:
                    if response.status != 200:
                        logger.debug(f"HTTP {response.status} for {url}")
                        return None
                    
                    content = await response.text()
                    
                    # Extract Greek text
                    extracted = self.extract_greek_from_html(content)
                    if not extracted:
                        logger.debug(f"No Greek text found in {url}")
                        return None
                    
                    # Calculate metrics
                    greek_text = extracted['greek_text']
                    word_count = len(greek_text.split())
                    greek_ratio = len([c for c in greek_text if '\u0370' <= c <= '\u03FF' or '\u1F00' <= c <= '\u1FFF']) / len(greek_text) if greek_text else 0
                    
                    result = ExtractedText(
                        tlg_id=author['tlg_id'],
                        work_id=work['work_id'],
                        author_name=author['name'],
                        work_title=work.get('title', f"Work {work['work_id']}"),
                        greek_text=greek_text,
                        paragraphs=extracted['paragraphs'],
                        url=url,
                        extraction_date=datetime.now().isoformat(),
                        greek_ratio=greek_ratio,
                        word_count=word_count,
                        extraction_method=extracted['method']
                    )
                    
                    self.processed_works.add(work_key)
                    return result
                    
            except asyncio.TimeoutError:
                wait_time = (2 ** attempt) * CONFIG["extraction"]["rate_limit"]["base_delay"]
                logger.warning(f"Timeout for {url}, retrying in {wait_time}s")
                await asyncio.sleep(wait_time)
            except Exception as e:
                logger.error(f"Error extracting {url}: {e}")
                return None
        
        return None
    
    async def extract_all_works(self):
        """Extract texts from all works in the catalogue."""
        authors = self.catalogue['authors']
        total_works = sum(len(author['works']) for author in authors)
        
        logger.info(f"Starting extraction of {total_works} works from {len(authors)} authors")
        
        processed_count = 0
        successful_count = 0
        
        for author in authors:
            logger.info(f"Processing author TLG{author['tlg_id']}: {author['name']} ({len(author['works'])} works)")
            
            for work in author['works']:
                try:
                    result = await self.extract_work(author, work)
                    if result:
                        self.results.append(result)
                        successful_count += 1
                        logger.debug(f"Extracted: TLG{result.tlg_id}.{result.work_id} ({result.word_count} words)")
                    
                    processed_count += 1
                    
                    # Progress logging
                    if processed_count % CONFIG["extraction"]["progress_log_frequency"] == 0:
                        logger.info(f"Progress: {processed_count}/{total_works} works processed ({successful_count} successful)")
                    
                    # Save checkpoint periodically
                    if processed_count % CONFIG["output"]["formats"]["json"]["checkpoint_frequency"] == 0:
                        self.save_checkpoint()
                    
                    # Respectful delay
                    await asyncio.sleep(CONFIG["extraction"]["rate_limit"]["base_delay"])
                    
                except Exception as e:
                    logger.error(f"Error processing work TLG{author['tlg_id']}.{work['work_id']}: {e}")
                    continue
        
        logger.info(f"Extraction complete: {successful_count}/{total_works} works extracted successfully")
    
    def save_checkpoint(self):
        """Save current extraction progress."""
        CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)
        checkpoint_file = CHECKPOINT_DIR / f"tlg_extraction_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        checkpoint_data = {
            'processed_works': list(self.processed_works),
            'results_count': len(self.results),
            'timestamp': datetime.now().isoformat()
        }
        
        with open(checkpoint_file, 'w', encoding='utf-8') as f:
            json.dump(checkpoint_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"Checkpoint saved: {checkpoint_file}")
    
    def save_results(self):
        """Save extracted texts in multiple formats."""
        OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
        
        # JSON format - complete data
        json_file = OUTPUT_DIR / CONFIG["output"]["formats"]["json"]["filename"]
        json_data = {
            'metadata': {
                'extraction_date': datetime.now().isoformat(),
                'total_works': len(self.results),
                'total_authors': len(set(r.tlg_id for r in self.results)),
                'total_words': sum(r.word_count for r in self.results),
                'source': 'TLG Corpus'
            },
            'texts': [
                {
                    'tlg_id': r.tlg_id,
                    'work_id': r.work_id,
                    'author_name': r.author_name,
                    'work_title': r.work_title,
                    'greek_text': r.greek_text,
                    'paragraphs': r.paragraphs,
                    'url': r.url,
                    'extraction_date': r.extraction_date,
                    'greek_ratio': r.greek_ratio,
                    'word_count': r.word_count,
                    'extraction_method': r.extraction_method
                }
                for r in self.results
            ]
        }
        
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, ensure_ascii=False, indent=2)
        
        # Plain text format - Greek text only
        text_file = OUTPUT_DIR / CONFIG["output"]["formats"]["text"]["filename"]
        with open(text_file, 'w', encoding='utf-8') as f:
            for result in self.results:
                f.write(f"=== TLG{result.tlg_id}.{result.work_id}: {result.author_name} - {result.work_title} ===\\n")
                f.write(result.greek_text)
                f.write("\\n\\n" + "="*80 + "\\n\\n")
        
        # Statistics
        stats_file = OUTPUT_DIR / CONFIG["output"]["formats"]["statistics"]["filename"]
        stats = {
            'extraction_date': datetime.now().isoformat(),
            'total_works_extracted': len(self.results),
            'total_authors': len(set(r.tlg_id for r in self.results)),
            'total_words': sum(r.word_count for r in self.results),
            'average_greek_ratio': sum(r.greek_ratio for r in self.results) / len(self.results) if self.results else 0,
            'extraction_methods': {
                method: len([r for r in self.results if r.extraction_method == method])
                for method in set(r.extraction_method for r in self.results)
            },
            'authors_by_work_count': {
                tlg_id: len([r for r in self.results if r.tlg_id == tlg_id])
                for tlg_id in sorted(set(r.tlg_id for r in self.results))
            }
        }
        
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)
        
        logger.info(f"Results saved:")
        logger.info(f"  JSON: {json_file}")
        logger.info(f"  Text: {text_file}")
        logger.info(f"  Statistics: {stats_file}")
    
    async def run(self):
        """Run the complete extraction process."""
        try:
            await self.setup()
            await self.extract_all_works()
            self.save_results()
        finally:
            await self.cleanup()

async def main():
    """Main entry point."""
    logger.info("=" * 60)
    logger.info("TLG Corpus Text Extraction")
    logger.info("Extracts Greek texts from all discovered TLG works")
    logger.info("=" * 60)
    
    extractor = TLGExtractor()
    await extractor.run()
    
    logger.info("=" * 60)
    logger.info("Extraction complete!")
    logger.info("=" * 60)

if __name__ == "__main__":
    asyncio.run(main())
